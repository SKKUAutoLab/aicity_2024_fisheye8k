# My add-in settings
task : detect
mode : train
model: yolo_nas_l
data : aic24_fisheye8k_of.yaml
imgsz: 1280

# Training settings
seed                  : 42
batch_size            : 16
workers               : 8
max_epochs            : 500
max_train_batches     : null   # For debug- when not None- will break out of inner train loop (i.e iterating over train_loader) when reaching this number of batches.
max_valid_batches     : null   # For debug- when not None- will break out of inner valid loop (i.e iterating over valid_loader) when reaching this number of batches.
multi_gpu             : "AUTO"
num_gpus              : 4

resume                : False
resume_path           : null
resume_strict_load    : False
load_opt_params       : True

initial_lr            : 0.0005
lr_mode               : "cosine"
lr_warmup_epochs      : 3
lr_warmup_steps       : 0
lr_cool_down_epochs   : 0
lr_updates            : []
lr_schedule_function  : null
warmup_mode           : "linear_epoch_step"
warmup_initial_lr     : 0.000001
cosine_final_lr_ratio : 0.1
step_lr_update_freq   : null

optimizer: "Adam"
optimizer_params:
    weight_decay: 0.0001
zero_weight_decay_on_bias_and_bn : True
batch_accumulate                 : 1  # number of batches to accumulate before every backward pass

ema: True
ema_params:
    decay     : 0.9
    decay_type: "threshold"

loss: PPYoloELoss

run_validation_freq   : 1
run_test_freq         : 1
train_metrics_list    : []
valid_metrics_list    :
    - DetectionMetrics_050:
        score_thres      : 0.1
        top_k_predictions: 1000
        num_cls          : null
        normalize_targets: True
        post_prediction_callback:
            _target_       : super_gradients.training.models.detection_models.pp_yolo_e.PPYoloEPostPredictionCallback
            score_threshold: 0.01
            nms_top_k      : 1000
            max_predictions: 300
            nms_threshold  : 0.7
    - DetectionMetrics_050_095:
        score_thres      : 0.1
        top_k_predictions: 1000
        num_cls          : null
        normalize_targets: True
        post_prediction_callback:
            _target_       : super_gradients.training.models.detection_models.pp_yolo_e.PPYoloEPostPredictionCallback
            score_threshold: 0.01
            nms_top_k      : 1000
            max_predictions: 300
            nms_threshold  : 0.7
metric_to_watch       : "mAP@0.50"
greater_metric_to_watch_is_better: True

save_model            : True
ckpt_name             : "last.pth"
ckpt_best_name        : "best.pth"
save_ckpt_epoch_list  : []            # indices where the ckpt will save automatically

sync_bn               : False
average_best_models   : True
clip_grad_norm        : null
mixed_precision       : False
silent_mode           : False  # Silents the Print outs


# torch_compile         : False         # Enable or disable use of torch.compile to optimize the model
# torch_compile_loss    : False         # Enable or disable use of torch.compile to optimize the loss
# torch_compile_options:
#     mode     : "reduce-overhead"      # Can be either “default”, “reduce-overhead” or “max-autotune”
#     fullgraph: False                  # Whether it is ok to break model into several subgraphs
#     dynamic  : False                  # Use dynamic shape tracing
#     backend  : "inductor"             # backend to be used
#     options  : null                   # A dictionary of options to pass to the backend.
#     disable  : False                  # Turn torch.compile() into a no-op for testing
# finetune: False
# 
# launch_tensorboard    : False
# save_tensorboard_to_s3: False
# tensorboard_port      : null
# tb_files_user_prompt  : False         # Asks User for Tensorboard Deletion Prompt
# dataset_statistics    : False         # add a dataset statistical analysis and sample images to tensorboard
# 
# log_installed_packages: True
# sg_logger             : "base_sg_logger"
# sg_logger_params:
#     tb_files_user_prompt   : False    # Asks User for Tensorboard Deletion Prompt
#     project_name           : ""
#     launch_tensorboard     : False
#     tensorboard_port       : None
#     save_checkpoints_remote: False    # upload checkpoint files to s3
#     save_tensorboard_remote: False    # upload tensorboard files to s3
#     save_logs_remote       : False
# 
# precise_bn                  : False
# precise_bn_batch_size       : null
# phase_callbacks             : null
# pre_prediction_callback     : null
# enable_qat                  : False
# 
# kill_ddp_pgroup_on_end      : True    # Whether to kill the DDP process group in the end of training.
# resume_from_remote_sg_logger: False   # When true, ckpt_name (checkpoint filename to resume, ckpt_latest.pth by
#                                       # default) will be downloaded into the experiment checkpoints directory prior to loading weights, then resumed
#                                       # from that checkpoint. The source is unique to every logger, and currently supported for WandB loggers only.
#                                       # Note that for this to work, the experiment must be ran with sg_logger_params.save_checkpoints_remote=True. For
#                                       # WandB loggers, one must also pass the run id through the wandb_id arg in sg_logger_params.
